{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árboles de decisiones para predecir si una persona sufrirá de un derrame cerebral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Leer el archivo stroke.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "# Se importan las librerias\n",
    "#----------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "# Url de los datos\n",
    "#----------------------------------------------------------\n",
    "\n",
    "url = (\"https://raw.githubusercontent.com/Yurani1143/Informe_Inteligencia_Artificial/main/stroke.csv\")\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# Se cargan los datos\n",
    "#----------------------------------------------------------\n",
    "datos = pd.read_csv(url, sep=\",\")\n",
    "datos.columns= [\"sexo\",\"edad\",\"hipertension\",\"enfermedades_cardiacas\",\"casado\",\"tipo_trabajo\",\"tipo_residencia\",\"nivel_glucosa_promedio\",\"indice_masa_corporal\",\"estado_tabaquismo\",\"derrame_cerebral\"]\n",
    "datos.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Seleccionar aleatoriamente el 80% del conjunto de datos para entrenar y el 20% restante para las pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "# Libreria sklearn\n",
    "#----------------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "N=len(datos) # cantidad de datos\n",
    "cTrain=int(N*0.8) # 80% para entrenar y 20% para probar\n",
    "cTest=N-cTrain \n",
    "\n",
    "print(N,cTrain,cTest)# cantidad de: datos completos, datos entrenados, datos para probar\n",
    "\n",
    "train_data,test_data= sklearn.model_selection.train_test_split(datos, train_size=cTrain, test_size=cTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "700 es el número de observaciones en la base de datos\n",
    "\n",
    "560 es el 80% de las observaciones de la base de datos que se van a utilizar para entrenar los datos\n",
    "\n",
    "140 es el 20% de las observaciones de la base de datos que se van a utilizar para probar el modelo de predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "# Dimensiones de los datos de entrenamiento\n",
    "#----------------------------------------------------------\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utilizar una estrategia para normalizar los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline para los atributos categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "#Librerias sklearn\n",
    "#----------------------------------------------------------\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 5 atributos categóricos\n",
    "cat_attribs = ['sexo','casado','tipo_trabajo','tipo_residencia','estado_tabaquismo']\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"cat_encoder\", OneHotEncoder(sparse=False))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline para los atributos númericos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 5 atributos numéricos\n",
    "num_attribs = ['edad','hipertension','enfermedades_cardiacas','nivel_glucosa_promedio','indice_masa_corporal']\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "#Libreria sklearn\n",
    "#----------------------------------------------------------\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", cat_pipeline, cat_attribs),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "# Se transforman los datos entrenados \n",
    "# Variables independientes\n",
    "#----------------------------------------------------------\n",
    "X_train = full_pipeline.fit_transform(train_data)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "# Variable dependiente -> derrame_cerebral\n",
    "#----------------------------------------------------------\n",
    "\n",
    "y_train = train_data[\"derrame_cerebral\"]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "# Se transforman los datos de prueba \n",
    "# Variables independientes\n",
    "#--------------------------------------------------------\n",
    "X_test = full_pipeline.transform(test_data) \n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "# Variable dependiente -> derrame_cerebral\n",
    "#----------------------------------------------------------\n",
    "y_test = test_data[\"derrame_cerebral\"]\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configurar los hiperparámetros del árbol de decisión de la siguiente manera: criterion=gini, splitter=best, y random_state=123. Obtener 10 árboles de decisión que resultan de modificar el hiperparámetro max_depth desde 5 hasta 50 con incrementos de 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "#Librerias sklearn.tree y sklearn.model_selection\n",
    "#----------------------------------------------------------\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "scores=[]\n",
    "scor=[]\n",
    "decision_tree = []\n",
    "accuracies = []\n",
    "max_depths = range(5, 55, 5)\n",
    "\n",
    "for max_depth in max_depths: \n",
    "    model_tree = DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\", max_depth=max_depth, random_state=123)\n",
    "    model_tree.fit(X_train, y_train)\n",
    "    y_pred = model_tree.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred) \n",
    "    accuracies.append(accuracy)\n",
    "    decision_tree.append(model_tree.score(X_test, y_test))\n",
    "    scores = cross_val_score(model_tree, X_train, y_train, cv=5,scoring='accuracy') \n",
    "    scor.append(scores.mean()) \n",
    "    print(\"----------------------------------\")\n",
    "    print(\"Puntajes de la validación cruzada:\")\n",
    "    print(scores)\n",
    "    print(f\"Promedio de los puntajes con max_depth={max_depth}\",\"->\" ,round(scores.mean(),4))\n",
    "    print(\"----------------------------------\")\n",
    "    \n",
    "#----------------------------------------------------------   \n",
    "# El índice del árbol con mayor exactitud\n",
    "best_tree_index = np.argmax(scor)\n",
    "#----------------------------------------------------------\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# Se imprime los hiperparámetros del árbol con mayor precisión\n",
    "print(\"Los hiperparámetros del árbol con mayor precisión son:\")\n",
    "print(\"max_depth:\", max_depths[best_tree_index])\n",
    "print(\"criterion: gini\")\n",
    "print(\"splitter: best\")\n",
    "print(\"random_state: 123\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Incluya en el notebook una tabla con el accuracy para los 10 árboles del punto anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "#Libreria pandas\n",
    "#----------------------------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "# Mostrar el accuracy de los 10 arboles construidos\n",
    "accuracies_CV = pd.DataFrame({\"max_depth\": range(5, 55, 5), \"accuracy\": scor})\n",
    "display(accuracies_CV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Repita el mismo procedimiento del punto 4 usando como hiperparámetros criterion=entropy, splitter=best, random_state=123, y variando el hiperparámetro max_depth desde 5 hasta 50 con incrementos de 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "#Librerias sklearn.tree y sklearn.model_selection\n",
    "#----------------------------------------------------------\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "scores_2=[]\n",
    "scor_2 =[]\n",
    "decision_tree_2 = []\n",
    "accuracies_7= []\n",
    "max_depths = range(5, 55, 5)\n",
    "\n",
    "for max_depth in max_depths: \n",
    "    model_tree_2 = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\", max_depth=max_depth, random_state=123)\n",
    "    model_tree_2.fit(X_train, y_train)\n",
    "    accuracy_7 = accuracy_score(y_test, y_pred) # exactitud de cada árbol\n",
    "    accuracies_7.append(accuracy_7)\n",
    "    decision_tree_2.append(model_tree_2.score(X_test, y_test))\n",
    "    scores_2 = cross_val_score(model_tree_2, X_train, y_train, cv=5,scoring='accuracy')\n",
    "    scor_2.append(scores_2.mean()) # promedio de los scores\n",
    "    print(\"----------------------------------\")\n",
    "    print(\"Puntajes de la validación cruzada:\")\n",
    "    print(scores_2)\n",
    "    print(f\"Promedio de los puntajes con max_depth={max_depth}\",\"->\" ,round(scores_2.mean(),4))\n",
    "    print(\"----------------------------------\")\n",
    "\n",
    "#----------------------------------------------------------   \n",
    "# El índice del árbol con mayor exactitud\n",
    "best_tree_index_2 = np.argmax(scor_2)\n",
    "#----------------------------------------------------------\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# Se imprime los hiperparámetros del árbol con mayor precisión\n",
    "print(\"Los hiperparámetros del árbol con mayor precisión son:\")\n",
    "print(\"max_depth:\", max_depths[best_tree_index_2])\n",
    "print(\"criterion: entropy\")\n",
    "print(\"splitter: best\")\n",
    "print(\"random_state: 123\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Incluya en el notebook una tabla con el accuracy para los 10 árboles del punto anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "#Libreria pandas\n",
    "#----------------------------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "# Mostrar el accuracy de los 10 arboles construidos \n",
    "accuracies_2 = pd.DataFrame({\"max_depth\": range(5, 55, 5), \"accuracy\": scor_2})\n",
    "display(accuracies_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Repita el mismo procedimiento del punto 4 usando como hiperparámetros criterion=entropy, splitter=random, random_state=123, y variando el hiperparámetro max_depth desde 5 hasta 50 con incrementos de 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "#Librerias sklearn.tree y sklearn.model_selection\n",
    "#----------------------------------------------------------\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "scores_3=[]\n",
    "scor_3=[]\n",
    "decision_tree_3 = []\n",
    "accuracies_9 = []\n",
    "max_depths = range(5, 55, 5)\n",
    "\n",
    "for max_depth in max_depths: \n",
    "    model_tree_3 = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"random\", max_depth=max_depth, random_state=123)\n",
    "    model_tree_3.fit(X_train, y_train)\n",
    "    accuracy_9 = accuracy_score(y_test, y_pred) \n",
    "    accuracies_9.append(accuracy_9)\n",
    "    decision_tree_3.append(model_tree_3.score(X_test, y_test))\n",
    "    scores_3 = cross_val_score(model_tree_3, X_train, y_train, cv=5,scoring='accuracy') \n",
    "    scor_3.append(scores_3.mean()) \n",
    "    print(\"----------------------------------\")\n",
    "    print(\"Puntajes de la validación cruzada:\")\n",
    "    print(scores_3)\n",
    "    print(f\"Promedio de los puntajes con max_depth={max_depth}\",\"->\" ,round(scores_3.mean(),4))\n",
    "    print(\"----------------------------------\")\n",
    "    \n",
    "#----------------------------------------------------------   \n",
    "# El índice del árbol con mayor exactitud\n",
    "best_tree_index_3 = np.argmax(scor_3)\n",
    "#----------------------------------------------------------\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# Se imprime los hiperparámetros del árbol con mayor precisión\n",
    "print(\"Los hiperparámetros del árbol con mayor precisión son:\")\n",
    "print(\"max_depth:\", max_depths[best_tree_index_3])\n",
    "print(\"criterion: entropy\")\n",
    "print(\"splitter: random\")\n",
    "print(\"random_state: 123\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Incluya en el notebook una tabla con el accuracy para los 10 árboles del punto anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "#Libreria pandas\n",
    "#----------------------------------------------------------\n",
    "import pandas as pd\n",
    "\n",
    "# Mostrar el accuracy de los 10 arboles construidos \n",
    "accuracies_3 = pd.DataFrame({\"max_depth\": range(5, 55, 5), \"accuracy\": scor_3})\n",
    "display(accuracies_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
